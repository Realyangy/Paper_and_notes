## Outrageously Large Neural Networks：The Sparsely-Gated Mixture-Of-Experts Layer

## （极其庞大的神经网络：稀疏门控专家混合层）



### 一.本文的工作

1.将条件计算运用于神经网络，提出稀疏门控专家混合层MoE，它包括多个前馈子网络（专家）和一个门控单元，对于每一个输入，门控网络会选择一些专家来处理该输入。

2.将一个有137B参数量的MoE层卷积式应用于堆叠的LSTM层之中，在语言建模和机器翻译的任务中取得SOTA效果。



### 二.MoE层的介绍

1.简单的门控网络：softmax门控网络

​	将输入与可训练的权重相乘，然后通过一层softmax，取概率较大的专家。

2.本文使用的门控网络：噪声Top-k门控网络

​	在softmax门控网络的基础上加上了稀疏性和噪声。在每个专家都有得分，而且还没有进入softmax之前，对专家得分向量加上噪声，然后只取top-k个得分最高的专家，其他专家的得分设置为负无穷（经过softmax之后会变为0）。这样之后才通过softmax。

​	具体过程可见论文公式（3）（4）（5）。

3.上述门控网络的训练：

​	每个专家的在经过softmax之前的得分值是输入和门控网络的参数相乘然后加上噪声得到的，所以可以将softmax得分视为门控网络参数的函数。它对于门控网络的参数是可导的，所以可以求出这些参数的梯度，用于更新这些参数；同时，梯度也会流入输入端（就是生成MoE层输入的上层网络，论文中应该是LSTM），用来更新输入端网络的参数。



### 三.条件计算的问题

#### （1）收缩batch问题

​	**问题来源：**使用较大的batch_size的目的是摊薄计算中的固定开销。如果每个样本只激活k个专家，那么对于有b个样本的batch来说，专家调用次数为kb。假设这些调用在n个专家上平均分配，则每个专家被调用kb/n次。也就是说，每个专家处理了kb/n个样本。对它们来说，实际batch_size就是kb/n，这远远小于b，由此导致batch收缩问题。

​	针对上述问题，如果使用超级大batch_size，又会出现显存爆炸的问题（本文方法就是这样做的）。本文提出如下方法解决这个问题。

##### 1.混合并行

​	**（数据同步）**对于模型中除了MoE层之外的其他层，还是使用数据并行的分布式训练机制，即各设备异步训练。但是当训练到达MoE层之前，所有数据需要同步，将整个batch合并在一起送入MoE层。**（模型存储）**对于其他层，所有设备上都有一份拷贝；对于MoE层，其中的每个专家只存储在某个设备上。如专家1只在GPU0上存储，专家2只在GPU3上。如果每个GPU上都存储所有专家，那显存会爆炸。**（混合并行）**对于其他层，各GPU上都有一份拷贝，且使用不同minibatch进行训练，相当于数据并行；对于MoE层，各GPU上都有不同专家，即MoE层的不同部分，相当于模型并行。

​	**解决方法分析：**门控网络对整个batch中的各个样本分别判断，决定将其送给哪个专家处理，每个专家处理的是大batch的子集。假设有d个设备，每个设备处理的minibatch大小为b，总的batch大小就是db。和之前分析一样，设每个样本只激活k个专家，则总调用次数是kdb。在n个专家上平均分配，则每个专家被调用kdb/n次，即实际batch_size是kdb/n。这比单卡的kb/n扩大d倍。

​	其实就是使用多卡，让显存变大，然后使用超级大batch_size。

##### 2.利用卷积

​	LSTM的每个时间步都会通过MoE层进行计算。本文将单个单个时间步通过MoE修改为并行计算：将LSTM的所有隐藏状态$[h_1,h_2,...,h_T]$同时输入MoE。MoE仍然会为每个隐藏状态分配专家进行计算，但是可以并行计算所有时间步。这样，MoE处理的数据量就从B个样本（每个样本有T个时间步）扩大为BT个，充分运用了GPU的并行计算能力。这种把所有LSTM输出一起计算的技巧称为卷积。

##### 3.将MoE融入循环结构

​	将LSTM或者RNN的权重矩阵之间更换为MoE层，但是这样当前时间步的MoE输入需要依赖于上一个时间步的MoE输出，无法使用上面的卷积技巧。

#### （2）网络带宽问题

​	不同设备上的专家需要接收到分配给自己的样本进行计算；在计算完成后，还要传输到原来的位置进行合并。这导致分布式训练中的数据传输（通信）成为问题。

​	**问题定义。**上述问题需要对比任务的AI和GPU的AI（AI解释可见下面）。本文中，任务的AI定义为专家的计算量和它需要发送和接受的数据量的比例，它表示在传输1byte数据的时间内，专家需要做的运算次数。
$$
Task\ AI=\frac{Experts \ FLOPs}{Inputs+Outputs}
$$
对于特定的GPU，它的峰值算力和显存带宽是设计好的，所以它们的比例（AI）也是固定的，表示在从显存或者主机内存获取1byte的时间内，GPU可以进行的运算次数。一般来说，GPU的计算能力远大于它的网络传输能力，如A100的该比例约为150。
$$
GPU\ AI=\frac{GPU\ Compute\ Capacity}{GPU\ NetWork\ BandWidth}
$$
要保证充分利用GPU，Task AI需要大于GPU AI，这意味着第一个任务来到时，GPU需要进行一定运算；在第二个任务来到时，GPU仍然在进行前一个任务的计算，而且计算完成后可以直接进行第二个任务的计算，中间没有空闲时间。

​	**本文中的做法。**每个专家都是只有一个隐藏层的神经网络（输入-隐藏-输出）。那么专家的计算量就是两个矩阵计算：输入-隐藏的I\*H、隐藏-输出的H\*O；专家的通信量是输入输出的和：I+O。那么任务AI即为（O是计算复杂度）：
$$
\frac{O(I \times H+H \times O)}{O(I+O)}=H
$$
所以任务AI可以使用隐藏层的维度来控制。



### 四.训练时的问题

#### （1）重要性分配不均

​	**问题定义。**MoE的训练中经常出现的问题：专家的不平衡使用，门控网络会经常给几个相同的专家分配较大的权重。出现的原因可能是：一开始这几个热门专家被多数样本上被选中，它们的实际batch_size更大（前面假设总调用次数在n个专家上均分，这里不平均），训练速度比其他专家快。这种现象会导致：门控网络频繁把任务分给热门专家，少数热门专家包揽全部任务，多数冷门专家无事可做。

​	**本文中的做法。**给每个专家赋予重要性。对于每个样本，门控网络都会给出权重向量，表示被选中的专家及其权重。专家的重要性定义为一个batch内的所有需要它处理的样本中，它的门控权重之和。为了使用专家重要性，还在训练的损失函数后增加了一项，记为重要性损失：
$$
L_{importance}(X)=\omega_{importance}\cdot CV(Importance(X))^2
$$
​	下面对这个损失函数进行解释。在一个batch的训练中，会得到n个专家的重要性向量$Importance(X)$。计算这组重要性的均值$\mu$和方差$\sigma$，求得变异系数（Coefficient of Variation，CV）$\frac{\sigma}{\mu}$，它刻画了这组数据的均衡程度，越小说明数据分布越平均。将其平方后乘以手动设置的系数$\omega_{importance}$，即为该batch的重要性损失。

#### （2）负载分配不均

​	**问题定义。**专家重要性是它被分配到的所有样本的权重和，那么会出现少样本大权重和多样本少权重，但是总和相等的情况。专家被分配到的样本数量可视为它的负载，某些专家负载重，对应GPU计算时间长；某些专家负载轻则计算时间短。在下一个batch开始时，所有GPU需要同步，这会导致负载小的GPU空闲。

​	**本文中的做法。**再加一个损失项$L_{load}(X)$，让各专家分得的样本个数也均等。由于每个专家分得的样本数是一个离散值，无法通过反向传播训练，所以本文为每个专家定义负载量的属性$Load(X)_i$，这是可以使用反向传播训练的连续值。

​	首先使用$P(x,i)$来衡量给定样本x的情况下，第i个专家被选中的概率。具体得到$P(x,i)$的方法如下。门控网络给出所有专家的原始得分，然后加噪得到加噪后得分$H(x)$。对第i个专家原始得分重新加噪为$H'(x_i)$，而其他专家仍使用加噪后的得分$H(x_k)(k\neq i)$。$P(x,i)$就是第i个专家重新加噪后被选中的概率。如果重新加噪后的得分$H'(x_i)$大于其他专家的得分的第K个，那么它就处于Top-K，即被选中。所以$P(x,i)$的计算公式为：
$$
P(x,i)=P(H'(x_i)>k^{th}\_excluding(H(x),k,i))
$$
$k^{th}\_excluding(H(x),k,i))$是指向量$H(x)$中不包括第i个在内的元素中第k大的元素。由此，可以将$P(x,i)$转化为标准正态分布的累计分布函数的形式（论文中公式（9））。专家i的在一个batch中的负载$Load(X)_i$即为各样本被分配到专家i的概率和：
$$
Load(X)_i=\sum_{x\in X} P(x,i)
$$
​	最终添加的损失项和重要性损失项形式类似，均为一个batch中各专家的负载量的变异系数平方与系数之积：
$$
L_{load}(X)=\omega_{load}\cdot CV(Load(X))^2
$$


### 五.本文使用的模型架构

​	使用堆叠LSTM+MoE网络架构，可以理解为LSTM+MoE+LSTM的结构。LSTM需要在每个时间步进行计算，得到当前时间步的隐藏状态$h_t$。MoE会对$h_t$进行计算，首先通过门控网络为其分配专家；然后这些专家被激活，对其进行前向计算：专家输出和门控权重加权求和作为输出$y_t$。最后通过dropout和残差连接，得到MoE层的输出$h_t+dropout(y_t)$。每个时间步都这样计算，得到长度为T的序列。这个序列会作为第二个LSTM的输入，再次进行上下文整合，最终输出。



### 六.生词解释

#### 1.较大的batch_size会摊薄计算开销

​	在每一个batch训练过程中，都有固定开销和可变开销。其中可变开销就是前向传播和反向传播的计算，计算量会随着batch_size的变化而正向变化，batch_size越大，可变开销越大。

​	主要的问题在固定开销上。当一个新的batch参与训练时，CUDA内核需要重新启动；新的数据会通过网络，得到网络中不同层的输出，这些中间层的输出都要保存到显存，用于计算梯度；计算得到的新的梯度也需要分配显存空间；优化器也要根据新的梯度更新参数。这些都是对当前batch的固定开销。batch越大，平均到每个样本上的固定开销就越小，可以理解为"摊薄"了固定开销。所以，batch_size越大，GPU的计算效率越高。

#### 2.分布式训练的两种方式：

##### 数据并行的分布式训练

​	对于较大的batch，会将其平均分为多个minibatch。参与训练的每个GPU上都有整个模型的拷贝。它们每次使用一个minibatch进行异步（见下面解释）训练，计算梯度，然后从参数服务器获取最新的参数，并进行更新。更新完成后将最新参数上传到参数服务器。

​	这里的异步是相对同步而言的：同步指的是每个设备完成自己的一轮前向-反向传播之后，需要等待其他设备。所有设备都完成一轮后，一起将梯度传递给参数服务器进行更新。异步指的是每个设备完成自己的一轮计算后，无需等待其他设备，可以直接将梯度传递给参数服务器进行更新。

##### 模型并行的分布式训练，又分为层级切分和算子级切分

​	**层级切分。**将模型不同的层放在不同的GPU上，如前5层放在GPU0上，后5层放在GPU1上。前向传播时，数据先过GPU0，再过GPU1；反向传播时反过来。这样做的缺点是会出现流水线空转问题：GPU0工作时GPU1空闲，反之亦然。

​	**算子级切分。**算子就是一步计算过程，如一次矩阵相乘、数据通过线性层、计算注意力机制等。假设某个线性层参数矩阵过大，可以将其拆分为两部分，分别放到两台设备上，GPU0负责前一半，GPU1负责后一半。输入向量分别和两个部分进行计算，然后拼接到一起作为结果输出。

#### 3.LSTM的输入输出

​	LSTM的输入是一个长度为T的序列$[x_1,x_2,...,x_T]$，通常可能是文本的嵌入向量序列。序列长度T可看作时间步的数量。在每个时间步t，LSTM会根据当前时间步的输入$x_t$和上一个时间步的隐藏状态$h_{t-1}$得到当前时间步的隐藏状态$h_t$。隐藏状态$h_t$包含了当前及其之前所有时间步的信息。LSTM的最终输出也是长度为T的序列$[h_1,h_2,...,h_T]$，最后一个隐藏状态$h_T$包含了整个序列的信息。

#### 4.FLOPs，Floating Point Operations，浮点运算次数

​	用来衡量神经网络的计算次数，1 FLOPs就是1次加法或者乘法。

#### 5.AI，Arithmetic Intensity, 算术强度

​	算术强度是算力和带宽的比值，单位是FLOPs/byte。算力的单位是FLOPs/s，即每秒完成的运算次数。带宽的单位是byte/s，即每秒传输的数据量。不同的GPU，不同的任务均有不同的AI。一般要将这两者进行对比，如果任务的AI大于GPU的AI，那么GPU在执行该任务时就不会出现空闲；反之GPU会空闲，导致任务效率低下。

​	以GPU为例，AI可以这样理解：从显存或者主机内存传输1byte数据的时间内，GPU可以做200（假设）次运算。如果在相同的这段时间内，任务需求只是做100（假设）次运算，那么GPU就会有一半的时间空闲，因为当前任务量已经完成，而接下来任务要使用的数据还没传输过来。相反的，如果任务需求是300次运算，那么当下一个任务数据传来时，GPU仍在计算上一个任务，而且结束时可以直接使用已经接收到的下一个任务数据。